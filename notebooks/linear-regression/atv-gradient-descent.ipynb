{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7f55b72-5fa3-402a-a59d-cdf9e67c97cc",
   "metadata": {},
   "source": [
    "# Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcec5c18-f598-491c-adbd-2aaa56bb3e7f",
   "metadata": {},
   "source": [
    "## Computing the minimum of a function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433c2a4e-7158-4711-a979-4ba75585f63b",
   "metadata": {},
   "source": [
    "Suppose we want to find the minimum of the function $$ f(x,y) = (x-47)^2 + (y - 0.1)^2 + 2. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a11837-db48-4811-b980-c071a0ac61fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------\n",
    "# Import packages\n",
    "#------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Change the plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc2d1cd-0bfc-44e7-890f-2d272ccdfd7e",
   "metadata": {},
   "source": [
    "We create the functions for our calculations such as $f$ and the gradient of $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d22a1-93dc-4b4e-ac42-727c5de33d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------\n",
    "# Create functions\n",
    "#------------------------\n",
    "# Input into the fuctions is vector X, where X = (x,y)\n",
    "# Create the function we want to find the minimum of\n",
    "def f(X):\n",
    "    #TODO: implement this part of code\n",
    "    return \n",
    "# Create the gradient vector\n",
    "def gradf(X):\n",
    "    #TODO: implement this part of code\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c92d4e-bd51-4e7c-a205-2fbddb8c8f83",
   "metadata": {},
   "source": [
    "We can plot $f$ as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef8a9bb-8767-41c6-b70b-de1562447d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement this part of code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e1dee-b057-4667-b749-6e891d646e4b",
   "metadata": {},
   "source": [
    "Now, we will start our gradient descent algorithm.  Here, we will start at the point $P=(80, 20)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fc230d-8ec4-4917-bd04-be216e68ecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement this part of code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b81469b-b953-487f-81ef-4840bb5a9852",
   "metadata": {},
   "source": [
    "We can plot the $(x,y)$ points from our algorithm as well.  First, we create a dataframe with all of the points from the xyValuesList."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d2b39-1257-42c6-ada8-2983f5fca37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement this part of code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4ceed6-51fb-4831-a454-fa3e24ce07ff",
   "metadata": {},
   "source": [
    "We will plot these point on a contour diagram to see how they move toward the minimum of $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca82835-61e4-4ec6-92d7-caf6e3e09ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement this part of code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12537414-2a4e-4008-8371-1695ba7c34a6",
   "metadata": {},
   "source": [
    "### Finding the Optimal Step Size for Gradient Descent\n",
    "\n",
    "### Symbolic Calculation of Optimal Step Size\n",
    "\n",
    "We want to find the optimal step size $\\alpha$ for a single gradient descent step from our starting point $\\mathbf{x}_0 = (80, 20)$. We use symbolic mathematics to derive an exact solution.\n",
    "\n",
    "First, we define our objective function:\n",
    "$$f(\\mathbf{x}) = (x - 47)^2 + (y - 0.1)^2 + 2$$\n",
    "\n",
    "The gradient of this function is:\n",
    "$$\\nabla f(\\mathbf{x}) = \\begin{pmatrix} 2(x - 47) \\\\ 2(y - 0.1) \\end{pmatrix}$$\n",
    "\n",
    "For our starting point $\\mathbf{x}_0 = (80, 20)$, the gradient is:\n",
    "$$\\nabla f(\\mathbf{x}_0) = \\begin{pmatrix} 2(80 - 47) \\\\ 2(20 - 0.1) \\end{pmatrix} = \\begin{pmatrix} 66 \\\\ 39.8 \\end{pmatrix}$$\n",
    "\n",
    "The next point in gradient descent is given by:\n",
    "$$\\mathbf{x}_1 = \\mathbf{x}_0 - \\alpha \\nabla f(\\mathbf{x}_0)$$\n",
    "\n",
    "Substituting our values:\n",
    "$$\\mathbf{x}_1 = \\begin{pmatrix} 80 \\\\ 20 \\end{pmatrix} - \\alpha \\begin{pmatrix} 66 \\\\ 39.8 \\end{pmatrix} = \\begin{pmatrix} 80 - 66\\alpha \\\\ 20 - 39.8\\alpha \\end{pmatrix}$$\n",
    "\n",
    "Now we define a function $\\phi(\\alpha)$ which is our objective function evaluated at $\\mathbf{x}_1$:\n",
    "$$\\phi(\\alpha) = f(\\mathbf{x}_1) = (80 - 66\\alpha - 47)^2 + (20 - 39.8\\alpha - 0.1)^2 + 2$$\n",
    "\n",
    "Simplifying:\n",
    "$$\\phi(\\alpha) = (33 - 66\\alpha)^2 + (19.9 - 39.8\\alpha)^2 + 2$$\n",
    "\n",
    "To find the optimal $\\alpha$, we take the derivative of $\\phi(\\alpha)$ with respect to $\\alpha$ and set it to zero:\n",
    "$$\\frac{d\\phi}{d\\alpha} = -2 \\cdot 66 \\cdot (33 - 66\\alpha) - 2 \\cdot 39.8 \\cdot (19.9 - 39.8\\alpha) = 0$$\n",
    "\n",
    "Simplifying:\n",
    "$$-132(33 - 66\\alpha) - 79.6(19.9 - 39.8\\alpha) = 0$$\n",
    "$$-4356 + 8712\\alpha - 1584.04 + 3168.08\\alpha = 0$$\n",
    "$$11880.08\\alpha = 5940.04$$\n",
    "$$\\alpha = 0.5$$\n",
    "\n",
    "Therefore, the optimal step size for the first iteration of gradient descent is 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aad12f-9b49-4ee3-b557-d54351edf466",
   "metadata": {},
   "source": [
    "### creating a function that perform the GD algorithm\n",
    "\n",
    "#### implement the GD algorithm, with options to fixed alpha, backtracking line search (Armijo and Wolfe Conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbec2eb-4ff1-4598-8909-597492fcbc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, grad_f, x0, alpha_type='fixed', alpha=0.01, \n",
    "                     max_steps=1000, tolerance=0.0001, \n",
    "                     c1=1e-4, rho=0.5, max_line_search_iter=20):\n",
    "    \"\"\"\n",
    "    Gradient descent optimization algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    f : function\n",
    "        The objective function to minimize, should take a numpy array as input.\n",
    "    grad_f : function\n",
    "        The gradient of the objective function, should take a numpy array as input.\n",
    "    x0 : numpy array\n",
    "        The starting point.\n",
    "    alpha_type : str, optional\n",
    "        Type of step size: 'fixed' or 'backtracking'. Default is 'fixed'.\n",
    "    alpha : float, optional\n",
    "        Step size for fixed alpha. Default is 0.01.\n",
    "    max_steps : int, optional\n",
    "        Maximum number of iterations. Default is 1000.\n",
    "    tolerance : float, optional\n",
    "        Convergence tolerance based on the norm of the difference. Default is 0.0001.\n",
    "    c1 : float, optional\n",
    "        Parameter for the Armijo condition in backtracking line search. Default is 1e-4.\n",
    "    rho : float, optional\n",
    "        Step size reduction factor for backtracking line search. Default is 0.5.\n",
    "    max_line_search_iter : int, optional\n",
    "        Maximum number of backtracking iterations. Default is 20.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X : numpy array\n",
    "        The final solution point.\n",
    "    f_values : list\n",
    "        Function values at each iteration.\n",
    "    path : list of numpy arrays\n",
    "        The path taken by the algorithm.\n",
    "    num_steps : int\n",
    "        Number of steps taken to converge.\n",
    "    \"\"\"\n",
    "    #TODO: implement this part of code\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b65e84-69ad-4eef-b5b3-98f5491c8763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Define your function and its gradient\n",
    "def f_example(X):\n",
    "    x, y = X\n",
    "    return (x-47)**2 + (y-0.1)**2 + 2\n",
    "\n",
    "def grad_f_example(X):\n",
    "    x, y = X\n",
    "    return np.array([2*(x-47), 2*(y-0.1)])\n",
    "\n",
    "#TODO: implement this part of code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313b8cc9-a80e-46f5-8c41-76705f30a726",
   "metadata": {},
   "source": [
    "#### show the results of your method for following cases:\n",
    "\n",
    "- $$f(x) = \\sum_{i=1}^{n-1}[100(x_{i+1}-x_i^2)^2+(1-x_i)^2], n = 5$$\n",
    "\n",
    "- $$ f(x,y) = (x+2y-7)^2 + (2x+y-5)^2 $$\n",
    "\n",
    "- $$ f(x,y) = (x^2 + y - 11)^2 + (x+y^2-7)^2 $$\n",
    "\n",
    "- $$ f(x,y) = 2x^2 - 1.05x^4 + \\frac{x^6}{6} + xy + y^2$$\n",
    "\n",
    "- $$ f(x,y) = sin(x+y) + (x-y)^2 - 1.5x + 2.5y + 1 $$\n",
    "\n",
    "You need to test, for all cases:\n",
    "\n",
    "different values for fixed alpha, different initial points.\n",
    "\n",
    "backtracking line search, only 1 initial point for each function, Armijo and Wolfe Conditions.\n",
    "\n",
    "present the results in a nice visual format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b06894-4524-4809-9aa6-5bc732195958",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate Gradient of given function\n",
    "#Using Forward Difference Method\n",
    "def grad(x):                    \n",
    "    value = np.zeros(len(x))    \n",
    "    xd = x.copy()\n",
    "    h = 10**-6\n",
    "    for j in range(len(x)):\n",
    "        xd[j] = x[j] + h\n",
    "        value[j] = (func(xd) - func(x))/h\n",
    "        xd = x.copy()\n",
    "    return value\n",
    " \n",
    "#Function for calculating Function Value \n",
    "def func(x):                     \n",
    "    return 20*(x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "\n",
    "result = gradient_descent(func, grad, [0, 0], alpha_type='fixed', alpha=0.01)\n",
    "plot_convergence_path(func, result[2], \"Gradient Descent with Fixed Alpha=0.01\", x_range = [-2,2], y_range = [-2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8807fa3b-1d5b-4375-987a-6f9b0fcd962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gradient_descent(func, grad, [0, 0], alpha_type='backtracking', alpha=1.0)\n",
    "plot_convergence_path(func, result[2], \"Gradient Descent with backtracking\", x_range = [-2,2], y_range = [-2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc80d68-5c01-4a70-bf5a-d7d19891f166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
