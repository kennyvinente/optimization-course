# Import libraries
import numpy as np
import matplotlib.pyplot as plt

# Warm-Up Exercise
def warmUpExercise():
    return np.eye(5)

# Test warmUpExercise
print("Running warmUpExercise...")
print("5x5 Identity Matrix:")
print(warmUpExercise())

# Loading and plotting data
data = np.loadtxt('data/ex1data1.txt', delimiter=',')
X = data[:, 0]  # Population size in 10,000s
y = data[:, 1]  # Profit in $10,000s
m = len(y)  # Number of training examples

# Plotting data
def plotData(X, y):
    plt.scatter(X, y, marker='x', c='r')
    plt.xlabel('Population size in 10,000s')
    plt.ylabel('Profit in $10,000s')
    plt.title('Profit vs Population')
    plt.show()

plotData(X, y)

# Cost function and gradient descent
def computeCost(X, y, theta):
    m = len(y)
    predictions = X @ theta
    sq_errors = (predictions - y) ** 2
    J = (1 / (2 * m)) * np.sum(sq_errors)
    return J

def gradientDescent(X, y, theta, alpha, iterations):
    m = len(y)
    J_history = np.zeros(iterations)
    
    for i in range(iterations):
        predictions = X @ theta
        errors = predictions - y
        theta = theta - (alpha / m) * (X.T @ errors)
        J_history[i] = computeCost(X, y, theta)
    
    return theta, J_history

# Prepare data for linear regression
X_with_intercept = np.column_stack((np.ones(m), X))
theta = np.zeros(2)  # Initialize fitting parameters
iterations = 1500
alpha = 0.01

# Test cost function
print("\nTesting the cost function...")
J = computeCost(X_with_intercept, y, theta)
print(f"With theta = [0 ; 0]\nCost computed = {J}")
print("Expected cost value (approx) 32.07")

J = computeCost(X_with_intercept, y, np.array([-1, 2]))
print(f"With theta = [-1 ; 2]\nCost computed = {J}")
print("Expected cost value (approx) 54.24")

# Run gradient descent
theta, J_history = gradientDescent(X_with_intercept, y, theta, alpha, iterations)
print("Theta found by gradient descent:")
print(theta)
print("Expected theta values (approx): [-3.6303, 1.1664]")

# Plot linear fit
plt.scatter(X, y, marker='x', c='r', label='Training data')
plt.plot(X, X_with_intercept @ theta, '-', label='Linear regression')
plt.xlabel('Population size in 10,000s')
plt.ylabel('Profit in $10,000s')
plt.legend()
plt.show()

# Predict values
predict1 = np.array([1, 3.5]) @ theta
predict2 = np.array([1, 7]) @ theta
print(f"For population = 35,000, we predict a profit of {predict1 * 10000}")
print(f"For population = 70,000, we predict a profit of {predict2 * 10000}")

# Visualizing J(theta_0, theta_1)
theta0_vals = np.linspace(-10, 10, 100)
theta1_vals = np.linspace(-1, 4, 100)
J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

for i, theta0 in enumerate(theta0_vals):
    for j, theta1 in enumerate(theta1_vals):
        t = np.array([theta0, theta1])
        J_vals[i, j] = computeCost(X_with_intercept, y, t)

# Surface plot
J_vals = J_vals.T
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
T0, T1 = np.meshgrid(theta0_vals, theta1_vals)
ax.plot_surface(T0, T1, J_vals, cmap='viridis')
ax.set_xlabel('theta_0')
ax.set_ylabel('theta_1')
ax.set_zlabel('Cost J')
plt.show()

# Contour plot
plt.contour(theta0_vals, theta1_vals, J_vals, levels=np.logspace(-2, 3, 20), cmap='viridis')
plt.xlabel('theta_0')
plt.ylabel('theta_1')
plt.plot(theta[0], theta[1], 'rx', markersize=10, linewidth=2)
plt.show()

# Load and prepare data for multiple variables
data = np.loadtxt('data/ex1data2.txt', delimiter=',')
X = data[:, :2]  # Features: size and number of bedrooms
y = data[:, 2]   # Target: price
m = len(y)       # Number of training examples

# Display first 10 examples of the dataset
print("First 10 examples from the dataset:")
for i in range(10):
    print(f"x = [{X[i, 0]:.0f}, {X[i, 1]:.0f}], y = {y[i]:.0f}")

# Normalize features
def featureNormalize(X):
    mu = np.mean(X, axis=0)
    sigma = np.std(X, axis=0)
    X_norm = (X - mu) / sigma
    return X_norm, mu, sigma

print("Normalizing Features...")
X_norm, mu, sigma = featureNormalize(X)

# Add intercept term to X
X = np.column_stack((np.ones(m), X_norm))

# Gradient Descent for multiple variables
def computeCostMulti(X, y, theta):
    return computeCost(X, y, theta)

def gradientDescentMulti(X, y, theta, alpha, num_iters):
    return gradientDescent(X, y, theta, alpha, num_iters)

# Running gradient descent
alpha = 0.01
num_iters = 400

theta = np.zeros(X.shape[1])  # Initialize fitting parameters
print("Running gradient descent...")

theta, J_history = gradientDescentMulti(X, y, theta, alpha, num_iters)

# Plot the convergence graph
plt.plot(range(1, len(J_history) + 1), J_history, '-b', linewidth=2)
plt.xlabel('Number of iterations')
plt.ylabel('Cost J')
plt.title('Convergence of Gradient Descent')
plt.show()

print("Theta computed from gradient descent:")
print(theta)

# Predicting price for a house with 1650 sq-ft and 3 bedrooms
normalized_features = (np.array([1650, 3]) - mu) / sigma
normalized_features = np.insert(normalized_features, 0, 1)  # Add intercept term
price = normalized_features @ theta

print(f"Predicted price of a 1650 sq-ft, 3 br house (using gradient descent): $ {price:.2f}")
print(f'Expected price (approx): 293081.46')

# Normal equations
def normalEqn(X, y):
    theta = np.linalg.inv(X.T @ X) @ X.T @ y
    return theta

print("Solving with normal equations...")
data = np.loadtxt('data/ex1data2.txt', delimiter=',')
X = data[:, :2]
y = data[:, 2]

# Add intercept term to X
X = np.column_stack((np.ones(len(y)), X))

theta = normalEqn(X, y)

print("Theta computed from the normal equations:")
print(theta)

# Predicting price for a house with 1650 sq-ft and 3 bedrooms using normal equations
price = np.array([1, 1650, 3]) @ theta
print(f"Predicted price of a 1650 sq-ft, 3 br house (using normal equations): $ {price:.2f}")
print(f'Expected price (approx): 293081.46')